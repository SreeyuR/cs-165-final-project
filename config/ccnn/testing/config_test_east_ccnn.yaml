default: &DEFAULT

  # Weights and biases
  wandb:
    log: False
    name: "ccnn-east-test" # If None, config will be used but you can override it here
    group: "ccnn-hyperparameter-optimization"
    project: "CCNN_Baseline"
    files_to_save:
      - "/Users/u235567/Desktop/cs-165-final-project/data.py"
      - "/Users/u235567/Desktop/cs-165-final-project/main.py"
      - "/Users/u235567/Desktop/cs-165-final-project/evaluation.py"
      - "/Users/u235567/Desktop/cs-165-final-project/model_params.py"
      - "/Users/u235567/Desktop/cs-165-final-project/model_types.py"
      - "/Users/u235567/Desktop/cs-165-final-project/model.py"
      - "/Users/u235567/Desktop/cs-165-final-project/plotting.py"
      - "/Users/u235567/Desktop/cs-165-final-project/printing.py"
      - "/Users/u235567/Desktop/cs-165-final-project/seq_to_label.py"
      - "/Users/u235567/Desktop/cs-165-final-project/seq_to_seq.py"
      - "/Users/u235567/Desktop/cs-165-final-project/utils.py"
  
  optuna: 
    use_optuna: False
    wandb_log: False
    num_trials: 15
    num_jobs: 1

  seed: 50 # 42

  # Training Parameters
  training:
    epochs: 300 #500
    save_checkpoint: True
    model_type: "ccnn"
    testing_for_debugging: False
    batch_size: 32 #64
    # for learing rate scheduler
    learning_rate_scheduler_target: 0.0008969475956462143
    learning_rate_adam: 0.001106752158436931
    learning_rate_init: 0.00000762542827083412 # for warmup
    warmup_steps: 37
    lr_scheduler_type: "StepLR" #"linear"
    step_size: 21 # 40 was not so good, go lower. 20 best for 100 epochs
    gamma: 0.4732569527914238
    # for adam optimizer
    adam_weight_decay: 0.00002187663726973759
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_eps: 1e-8
    # for FNO model
    curriculum_steps: 0
    mlp_dropout: 0
    n_modes: 16
    hidden_channels: 64
    n_layers: 4
    
    reg_to_class: # from regression to classification
      threshold: 0.6
      softmax_before_mean: False # if False, the softmax is done after mean

  ccnn:
    # Net config
    no_hidden: 114
    no_blocks: 4
    no_stages: -1
    dropout: 0.10361459532592666
    dropout_in: 0.10835582289430176
    dropout_type: "Dropout1d"
    norm: "BatchNorm"
    nonlinearity: "GELU"
    block_width_factors: [0.0]
    block_type: "S4"
    block_prenorm: true
    downsampling: []
    downsampling_size: -1
    # Kernel config
    kernel_type: "MAGNet"
    kernel_no_hidden: 32
    kernel_no_layers: 1
    kernel_omega_0: 1076.5150341742096
    kernel_input_scale: 0.4297985244600292
    kernel_bias: true
    kernel_size: "same"
    kernel_chang_initialize: true
    kernel_norm: "Identity"
    kernel_nonlinearity: "Identity"
    kernel_init_spatial_value: 0.918473633564097
    kernel_num_edges: -1
    kernel_bottleneck_factor: -1
    # Conv config
    conv_type: "SeparableFlexConv"
    conv_causal: true
    conv_use_fft: true
    conv_bias: true
    conv_padding: "same"
    conv_stride: 1
    conv_cache: false
    # Mask config
    mask_type: "gaussian"
    mask_init_value: 0.1405215644945351
    mask_threshold: 0.18391164454819375
    mask_dynamic_cropping: true
    mask_temperature: 0.0
    mask_learn_mean: false

  balance_classes:
    balanced_loss: True
    pos_weight_emphasis_factor: 1.2028744078403348 #1.5 will weight disruptions more than ratio; see if undersampling makes is enough first.
    undersample: False
    undersample_ratio: 1.4609225944801068 #2.0 number of times there are MORE non-disruptions than disruptions
    data_augmentation_windowing: False 
    # Ratio of augmented data to original data.
    data_augmentation_ratio: 2
    disruptivity_distance_window_length: 8 # for sliding window

  # Dataset related Parameters
  data:
    filename: "Full_HDL_dataset_unnormalized_no_nan_column_names_w_shot_and_time.pickle"
    folder: "" #"tokamak/Transformers-Plasma-Disruption-Prediction-autoformer/Model_training"
    dataset_type: "seq_to_label" #"state"
    max_length: 2048
    scaling_type: "standard" #"robust" "none"
    # used to determine deterministic label of shots with probabilistic label
    cmod_hyperparameter_non_disruptions: 0.1
    cmod_hyperparameter_disruptions: 0.9
    d3d_hyperparameter_non_disruptions: 0.1
    d3d_hyperparameter_disruptions: 0.9
    east_hyperparameter_non_disruptions: 0.05
    east_hyperparameter_disruptions: 0.92
    end_cutoff_timesteps: 5 #8 # microseconds
    end_cutoff_timesteps_test: 8
    case_number: 3 # train on all three tokamaks and test on just cmod
    tau_cmod: 12 # TODO: ASK: Is this a time constant associated with tokamak?
    tau_d3d: 35
    tau_east: 75
    standardize_disruptivity_plot_length: False
    use_smoothed_tau: True
    data_context_length: 100
    fix_sampling: True # equal spaced timesteps, but can vary across machines
    standardize_sampling_rate: False # whether to force all machines to have same sampling rate for each timestep. If true, resample rate defaults to .005
    smooth_v_loop: True
    v_loop_smoother: 10
    uniform_seq_length: True
    probabilistic_labels: True
    delta_t: 900  # uniform sequence length, only used if uniform_seq_length is True

  # Evaluation Parameters
  eval:
    eval_high_thresh: 0.8
    eval_low_thresh: 0.5
    eval_hysteresis: 2
    sweep_id: None
    unrolled_smoothing: 10
